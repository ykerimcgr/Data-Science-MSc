{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swLDp-GV7iQg"
   },
   "source": [
    "# Week 3: Basic Document Classification (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLh_fS4P7iQn"
   },
   "source": [
    "## Overview \n",
    "In labs this week (and next), the focus will be on the application of sentiment analysis. You will be using a corpus of **movie reviews**.\n",
    "\n",
    "You will be exploring various techniques that can be used to classify the sentiment of the movie reviews as either positive or negative. \n",
    "\n",
    "You will be developing your own **Word List** and **Naïve Bayes** classifiers and then comparing them to the **NLTK Naïve Bayes** classifier.\n",
    "\n",
    "First, we will need to download the movie_review corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1633608836524,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "3W2AdikDqe5G",
    "outputId": "09a883ae-ff87-4614-c768-54cbf75086d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/kerimciger/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kerimciger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0uVSM40qe5I"
   },
   "source": [
    "The movie_reviews corpus reader provides a number of useful methods:\n",
    "   * .categories()\n",
    "   * .fileids()\n",
    "   * .words()\n",
    "   \n",
    "First, we can use `.categories()` to check the set of labels with which the reviews have been labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1633608690904,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "N3bRgahNqe5J",
    "outputId": "1b4ffe87-116a-4c6a-dbd8-402be020dbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INtoThcCqe5J"
   },
   "source": [
    "We can use `.fileids()` to get all of the file names associated with a particular category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1633608693935,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "LZ00H5mdqe5K",
    "outputId": "72c81f08-5e09-48fb-ee28-edabf6b098b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews is 1000\n",
      "The number of negative reviews is 1000\n"
     ]
    }
   ],
   "source": [
    "pos_review_ids=movie_reviews.fileids('pos')\n",
    "neg_review_ids=movie_reviews.fileids('neg')\n",
    "\n",
    "print(\"The number of positive reviews is {}\".format(len(pos_review_ids)))\n",
    "print(\"The number of negative reviews is {}\".format(len(neg_review_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoHxpQ6Sqe5L"
   },
   "source": [
    "We can use `.words()` to get back word-tokenised reviews.  The argument to `.words()` is the file id of an individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1633608699333,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "KMvFYBPyqe5M",
    "outputId": "9c268cd8-2b57-4e12-f972-e2e0627ff987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['films', 'adapted', 'from', 'comic', 'books', 'have', ...],\n",
       " ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(pos_review_ids[0]), movie_reviews.words(neg_review_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1633608702334,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "68LWdfszqe5M",
    "outputId": "dc8508d7-cea0-4a4b-b0be-75589274c056"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(movie_reviews.words(pos_review_ids[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmkD_UEKqe5N"
   },
   "source": [
    "Note, the object returned by `movie_reviews.words()` looks a lot like a list (and behaves a lot like a list) - but it is actually a `StreamBackedCorpusView`.  This essentially means it is not necessarily all in memory  - it is retrieved from disk as needed.  If you want to see all of the words at once then you can convert it to a list using the `list()` constructor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1633608711002,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "XwmV4eh9qe5O",
    "outputId": "269f1303-8588-4d4c-d4c4-fc9899a7ba89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'from', 'comic', 'books', 'have', 'had', 'plenty', 'of', 'success', ',', 'whether', 'they', \"'\", 're', 'about', 'superheroes', '(', 'batman', ',', 'superman', ',', 'spawn', ')', ',', 'or', 'geared', 'toward', 'kids', '(', 'casper', ')', 'or', 'the', 'arthouse', 'crowd', '(', 'ghost', 'world', ')', ',', 'but', 'there', \"'\", 's', 'never', 'really', 'been', 'a', 'comic', 'book', 'like', 'from', 'hell', 'before', '.', 'for', 'starters', ',', 'it', 'was', 'created', 'by', 'alan', 'moore', '(', 'and', 'eddie', 'campbell', ')', ',', 'who', 'brought', 'the', 'medium', 'to', 'a', 'whole', 'new', 'level', 'in', 'the', 'mid', \"'\", '80s', 'with', 'a', '12', '-', 'part', 'series', 'called', 'the', 'watchmen', '.', 'to', 'say', 'moore', 'and', 'campbell', 'thoroughly', 'researched', 'the', 'subject', 'of', 'jack', 'the', 'ripper', 'would', 'be', 'like', 'saying', 'michael', 'jackson', 'is', 'starting', 'to', 'look', 'a', 'little', 'odd', '.', 'the', 'book', '(', 'or', '\"', 'graphic', 'novel', ',', '\"', 'if', 'you', 'will', ')', 'is', 'over', '500', 'pages', 'long', 'and', 'includes', 'nearly', '30', 'more', 'that', 'consist', 'of', 'nothing', 'but', 'footnotes', '.', 'in', 'other', 'words', ',', 'don', \"'\", 't', 'dismiss', 'this', 'film', 'because', 'of', 'its', 'source', '.', 'if', 'you', 'can', 'get', 'past', 'the', 'whole', 'comic', 'book', 'thing', ',', 'you', 'might', 'find', 'another', 'stumbling', 'block', 'in', 'from', 'hell', \"'\", 's', 'directors', ',', 'albert', 'and', 'allen', 'hughes', '.', 'getting', 'the', 'hughes', 'brothers', 'to', 'direct', 'this', 'seems', 'almost', 'as', 'ludicrous', 'as', 'casting', 'carrot', 'top', 'in', ',', 'well', ',', 'anything', ',', 'but', 'riddle', 'me', 'this', ':', 'who', 'better', 'to', 'direct', 'a', 'film', 'that', \"'\", 's', 'set', 'in', 'the', 'ghetto', 'and', 'features', 'really', 'violent', 'street', 'crime', 'than', 'the', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', '?', 'the', 'ghetto', 'in', 'question', 'is', ',', 'of', 'course', ',', 'whitechapel', 'in', '1888', 'london', \"'\", 's', 'east', 'end', '.', 'it', \"'\", 's', 'a', 'filthy', ',', 'sooty', 'place', 'where', 'the', 'whores', '(', 'called', '\"', 'unfortunates', '\"', ')', 'are', 'starting', 'to', 'get', 'a', 'little', 'nervous', 'about', 'this', 'mysterious', 'psychopath', 'who', 'has', 'been', 'carving', 'through', 'their', 'profession', 'with', 'surgical', 'precision', '.', 'when', 'the', 'first', 'stiff', 'turns', 'up', ',', 'copper', 'peter', 'godley', '(', 'robbie', 'coltrane', ',', 'the', 'world', 'is', 'not', 'enough', ')', 'calls', 'in', 'inspector', 'frederick', 'abberline', '(', 'johnny', 'depp', ',', 'blow', ')', 'to', 'crack', 'the', 'case', '.', 'abberline', ',', 'a', 'widower', ',', 'has', 'prophetic', 'dreams', 'he', 'unsuccessfully', 'tries', 'to', 'quell', 'with', 'copious', 'amounts', 'of', 'absinthe', 'and', 'opium', '.', 'upon', 'arriving', 'in', 'whitechapel', ',', 'he', 'befriends', 'an', 'unfortunate', 'named', 'mary', 'kelly', '(', 'heather', 'graham', ',', 'say', 'it', 'isn', \"'\", 't', 'so', ')', 'and', 'proceeds', 'to', 'investigate', 'the', 'horribly', 'gruesome', 'crimes', 'that', 'even', 'the', 'police', 'surgeon', 'can', \"'\", 't', 'stomach', '.', 'i', 'don', \"'\", 't', 'think', 'anyone', 'needs', 'to', 'be', 'briefed', 'on', 'jack', 'the', 'ripper', ',', 'so', 'i', 'won', \"'\", 't', 'go', 'into', 'the', 'particulars', 'here', ',', 'other', 'than', 'to', 'say', 'moore', 'and', 'campbell', 'have', 'a', 'unique', 'and', 'interesting', 'theory', 'about', 'both', 'the', 'identity', 'of', 'the', 'killer', 'and', 'the', 'reasons', 'he', 'chooses', 'to', 'slay', '.', 'in', 'the', 'comic', ',', 'they', 'don', \"'\", 't', 'bother', 'cloaking', 'the', 'identity', 'of', 'the', 'ripper', ',', 'but', 'screenwriters', 'terry', 'hayes', '(', 'vertical', 'limit', ')', 'and', 'rafael', 'yglesias', '(', 'les', 'mis', '?', 'rables', ')', 'do', 'a', 'good', 'job', 'of', 'keeping', 'him', 'hidden', 'from', 'viewers', 'until', 'the', 'very', 'end', '.', 'it', \"'\", 's', 'funny', 'to', 'watch', 'the', 'locals', 'blindly', 'point', 'the', 'finger', 'of', 'blame', 'at', 'jews', 'and', 'indians', 'because', ',', 'after', 'all', ',', 'an', 'englishman', 'could', 'never', 'be', 'capable', 'of', 'committing', 'such', 'ghastly', 'acts', '.', 'and', 'from', 'hell', \"'\", 's', 'ending', 'had', 'me', 'whistling', 'the', 'stonecutters', 'song', 'from', 'the', 'simpsons', 'for', 'days', '(', '\"', 'who', 'holds', 'back', 'the', 'electric', 'car', '/', 'who', 'made', 'steve', 'guttenberg', 'a', 'star', '?', '\"', ')', '.', 'don', \"'\", 't', 'worry', '-', 'it', \"'\", 'll', 'all', 'make', 'sense', 'when', 'you', 'see', 'it', '.', 'now', 'onto', 'from', 'hell', \"'\", 's', 'appearance', ':', 'it', \"'\", 's', 'certainly', 'dark', 'and', 'bleak', 'enough', ',', 'and', 'it', \"'\", 's', 'surprising', 'to', 'see', 'how', 'much', 'more', 'it', 'looks', 'like', 'a', 'tim', 'burton', 'film', 'than', 'planet', 'of', 'the', 'apes', 'did', '(', 'at', 'times', ',', 'it', 'seems', 'like', 'sleepy', 'hollow', '2', ')', '.', 'the', 'print', 'i', 'saw', 'wasn', \"'\", 't', 'completely', 'finished', '(', 'both', 'color', 'and', 'music', 'had', 'not', 'been', 'finalized', ',', 'so', 'no', 'comments', 'about', 'marilyn', 'manson', ')', ',', 'but', 'cinematographer', 'peter', 'deming', '(', 'don', \"'\", 't', 'say', 'a', 'word', ')', 'ably', 'captures', 'the', 'dreariness', 'of', 'victorian', '-', 'era', 'london', 'and', 'helped', 'make', 'the', 'flashy', 'killing', 'scenes', 'remind', 'me', 'of', 'the', 'crazy', 'flashbacks', 'in', 'twin', 'peaks', ',', 'even', 'though', 'the', 'violence', 'in', 'the', 'film', 'pales', 'in', 'comparison', 'to', 'that', 'in', 'the', 'black', '-', 'and', '-', 'white', 'comic', '.', 'oscar', 'winner', 'martin', 'childs', \"'\", '(', 'shakespeare', 'in', 'love', ')', 'production', 'design', 'turns', 'the', 'original', 'prague', 'surroundings', 'into', 'one', 'creepy', 'place', '.', 'even', 'the', 'acting', 'in', 'from', 'hell', 'is', 'solid', ',', 'with', 'the', 'dreamy', 'depp', 'turning', 'in', 'a', 'typically', 'strong', 'performance', 'and', 'deftly', 'handling', 'a', 'british', 'accent', '.', 'ians', 'holm', '(', 'joe', 'gould', \"'\", 's', 'secret', ')', 'and', 'richardson', '(', '102', 'dalmatians', ')', 'log', 'in', 'great', 'supporting', 'roles', ',', 'but', 'the', 'big', 'surprise', 'here', 'is', 'graham', '.', 'i', 'cringed', 'the', 'first', 'time', 'she', 'opened', 'her', 'mouth', ',', 'imagining', 'her', 'attempt', 'at', 'an', 'irish', 'accent', ',', 'but', 'it', 'actually', 'wasn', \"'\", 't', 'half', 'bad', '.', 'the', 'film', ',', 'however', ',', 'is', 'all', 'good', '.', '2', ':', '00', '-', 'r', 'for', 'strong', 'violence', '/', 'gore', ',', 'sexuality', ',', 'language', 'and', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "print(list(movie_reviews.words(pos_review_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXWztGUZqe5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUYw61N7iQo"
   },
   "source": [
    "## Creating training and testing sets\n",
    "You will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance. \n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "> Look through the code in the following cell, reading the comments and making sure that you understand each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1633608728313,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "HsMcMo5e7iQp"
   },
   "outputs": [],
   "source": [
    "import random # have a look at the documentation at https://docs.python.org/3/library/random.html \n",
    "\n",
    "\n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given collection of items and ratio:\n",
    "     - partitions the collection into training and testing, where the proportion in training is ratio,\n",
    "\n",
    "    :param data: A list (or generator) of documents or doc ids\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data)  #Found out number of samples present.  data could be a list or a generator\n",
    "    train_indices = random.sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Other items are testing indices\n",
    " \n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    " \n",
    "    return (train, test)                       #Return split data\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exampla dataset created to virtualize the split_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_list = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"score\": 85},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"score\": 90},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"score\": 75},\n",
    "    {\"id\": 4, \"name\": \"David\", \"score\": 92},\n",
    "    {\"id\": 5, \"name\": \"Eve\", \"score\": 88},\n",
    "    {\"id\": 6, \"name\": \"Frank\", \"score\": 79},\n",
    "    {\"id\": 7, \"name\": \"Grace\", \"score\": 94},\n",
    "    {\"id\": 8, \"name\": \"Hannah\", \"score\": 83},\n",
    "    {\"id\": 9, \"name\": \"Ivy\", \"score\": 77},\n",
    "    {\"id\": 10, \"name\": \"Jack\", \"score\": 80}\n",
    "]\n",
    "\n",
    "df_data = pd.DataFrame(data_list)\n",
    "\n",
    "train_data, test_data = split_data(data_list, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu1pTJhr7iQu"
   },
   "source": [
    "Now we can use this function to create training and testing data.  First, we need to create 4 lists:\n",
    "    * file ids  of positive docs to go in the training data\n",
    "    * file ids of positive docs to go in the testing data\n",
    "    * file ids of negative docs to go in the training data\n",
    "    * file ids of negative docs to go in the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1633608731133,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "QUVZGOpJ7iQv"
   },
   "outputs": [],
   "source": [
    "random.seed(41)  #set the random seeds so these random splits are always the same\n",
    "pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
    "neg_train_ids, neg_test_ids = split_data(neg_review_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQjhg51zqe5R"
   },
   "source": [
    "Now, we want to create our labelled data sets.   We need to associate each review with its label so that later we can shuffle up all of the training data (and the testing data)\n",
    "\n",
    "### Exercise 1\n",
    "Write some python code which will construct a training set (`training`) and a test set (`testing`) from the data.  Each set should be a list of pairs where each pair is a list of words and a label, as below:\n",
    "\n",
    "<code>[([list,of,words],'label'),([list,of,words],'label'),...]</code>\n",
    "\n",
    "Hint:  You can do this with 4 list comprehensions and list concatenation.\n",
    "\n",
    "Check the size of `training` and `testing`.  Using a 70\\% split, how many should be in each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1633608755893,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "yP5wctINNDUS"
   },
   "outputs": [],
   "source": [
    "training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
    "testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633608756520,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "kA8AJhWLqe5S",
    "outputId": "43a23021-a019-4919-fcb5-7f24c29e7a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['melvin', 'udall', 'is', 'a', 'heartless', 'man', '.', ...], 'pos'),\n",
       " (['why', 'do', 'people', 'hate', 'the', 'spice', ...], 'pos'),\n",
       " (['jerry', 'springer', 'has', 'got', 'nothing', 'on', ...], 'pos'),\n",
       " (['i', 'don', \"'\", 't', 'know', 'what', 'movie', 'the', ...], 'pos'),\n",
       " (['ingredients', ':', 'down', '-', 'on', '-', 'his', ...], 'pos')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))\n",
    "\n",
    "training[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3SLme3Oqe5S"
   },
   "source": [
    "## Document Representations\n",
    "\n",
    "Currently, each review / document is represented as a list of tokens.  In many simple applications, the order of words in a document is deemed irrelevant and we use a bag-of-words representation of the document.  We can create a bag-of-words using a dictionary (as we did in Lab_2_2 when considering the size of the vocabulary) or we can use a library function such as FreqDist from nltk.probability (or Counter from Collections).  In the cell below, I generate the bag-of-words for the first review in the training set using nltk's FreqDist.  You can think of this as like a dictionary but with extra benefits.  For example, later on in the lab, we will see it has useful methods which allow the document representations to be added and subtracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1633608763968,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "zRTriMZIqe5S",
    "outputId": "76fecb28-850f-4239-857a-97f07c1df5e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "doc1 = FreqDist(training[0][0])\n",
    "doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4qdRQEeqe5T"
   },
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Write code to use FreqDist to construct a bag-of-words representation for each document in the training and testing sets.  Store the results in two lists, `training_basic` and `testing_basic`.  Don't lost the annotations as to whether each review is positive or negative!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2849,
     "status": "ok",
     "timestamp": 1633608787302,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "8Pesd-Rjqe5T"
   },
   "outputs": [],
   "source": [
    "training_basic=[(FreqDist(wordlist),label) for (wordlist,label) in training]\n",
    "testing_basic=[(FreqDist(wordlist),label) for (wordlist,label) in testing]\n",
    "\n",
    "#training_basic=[(FreqDist(item[0]),item[1]) for item in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633608787306,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "8-sFdi8Jqe5T",
    "outputId": "5af7b03a-2588-4de7-b2de-4af99e9dfc14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({',': 24, '.': 18, 'and': 11, 'a': 9, 'to': 8, 'the': 7, 'melvin': 6, 'his': 6, \"'\": 6, 's': 6, ...}),\n",
       " 'pos')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_basic[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJd0x75aqe5U"
   },
   "source": [
    "You will notice of course that many of the words in your representations of documents are punctuation and stopwords.  This is because we haven't done any pre-processing of the wordlists.\n",
    "\n",
    "### Exercise 2.2\n",
    "\n",
    "Decide which of the following pre-processing steps to apply to the word lists:-\n",
    "* case normalisation\n",
    "* number normalisation\n",
    "* punctuation removal\n",
    "* stopword removal\n",
    "* stemming / lemmatisation\n",
    "\n",
    "\n",
    "Apply these preprocessing steps to the original wordlist representations (stored in `training` and `testing`).  Then recreate the bag-of-words representations, storing the results in `training_norm` and `testing_norm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1633608847683,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "DtkW4fzYqe5U",
    "outputId": "e3e2dba9-799a-4599-81a1-6ef5bb0db9a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['melvin',\n",
       " 'udall',\n",
       " 'heartless',\n",
       " 'man',\n",
       " 'spends',\n",
       " 'days',\n",
       " 'inside',\n",
       " 'spacious',\n",
       " 'manhattan',\n",
       " 'apartment',\n",
       " 'writing',\n",
       " 'romance',\n",
       " 'novels',\n",
       " 'also',\n",
       " 'seems',\n",
       " 'melvin',\n",
       " 'never',\n",
       " 'change',\n",
       " 'one',\n",
       " 'day',\n",
       " 'dines',\n",
       " 'ar',\n",
       " 'favorite',\n",
       " 'restaurant',\n",
       " 'little',\n",
       " 'mean',\n",
       " 'normal',\n",
       " 'waitress',\n",
       " 'waittress',\n",
       " 'serve',\n",
       " 'carol',\n",
       " 'played',\n",
       " 'perfection',\n",
       " 'lovely',\n",
       " 'sexy',\n",
       " 'helen',\n",
       " 'hunt',\n",
       " 'threatens',\n",
       " 'serve',\n",
       " 'shut',\n",
       " 'asthmatic',\n",
       " 'son',\n",
       " 'shut',\n",
       " 'make',\n",
       " 'matters',\n",
       " 'considerably',\n",
       " 'worse',\n",
       " 'melvin',\n",
       " 'obsessive',\n",
       " 'compulsive',\n",
       " 'disorder',\n",
       " 'one',\n",
       " 'day',\n",
       " 'gay',\n",
       " 'artist',\n",
       " 'neighbor',\n",
       " 'simon',\n",
       " 'greg',\n",
       " 'kinear',\n",
       " 'talk',\n",
       " 'soup',\n",
       " 'fame',\n",
       " 'oscar',\n",
       " 'worthy',\n",
       " 'role',\n",
       " 'dog',\n",
       " 'threatens',\n",
       " 'dismiss',\n",
       " 'melvon',\n",
       " 'door',\n",
       " 'dog',\n",
       " 'meets',\n",
       " 'garbage',\n",
       " 'chute',\n",
       " 'soon',\n",
       " 'simon',\n",
       " 'sadly',\n",
       " 'beaten',\n",
       " 'thieveing',\n",
       " 'burglars',\n",
       " 'ray',\n",
       " 'cuba',\n",
       " 'gooding',\n",
       " 'jr',\n",
       " 'simon',\n",
       " 'agent',\n",
       " 'takes',\n",
       " 'dog',\n",
       " 'verdell',\n",
       " 'melvin',\n",
       " 'melvin',\n",
       " 'dogsit',\n",
       " 'dog',\n",
       " 'rather',\n",
       " 'heartwrenching',\n",
       " 'car',\n",
       " 'trip',\n",
       " 'involves',\n",
       " 'simon',\n",
       " 'carol',\n",
       " 'melvin',\n",
       " 'learns',\n",
       " 'emerge',\n",
       " 'cantakerous',\n",
       " 'shell',\n",
       " 'jack',\n",
       " 'nicholson',\n",
       " 'gives',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'oscar',\n",
       " 'caliber',\n",
       " 'performance',\n",
       " 'film',\n",
       " 'cynical',\n",
       " 'lead',\n",
       " 'back',\n",
       " 'work',\n",
       " 'playing',\n",
       " 'goofy',\n",
       " 'u',\n",
       " 'president',\n",
       " 'comic',\n",
       " 'book',\n",
       " 'villian',\n",
       " 'instead',\n",
       " 'playing',\n",
       " 'jack',\n",
       " 'nicjolson',\n",
       " 'playing',\n",
       " 'full',\n",
       " 'force',\n",
       " 'adds',\n",
       " 'year',\n",
       " 'funniest',\n",
       " 'comedy',\n",
       " 'creative',\n",
       " 'witty',\n",
       " 'scathing',\n",
       " 'film',\n",
       " 'james',\n",
       " 'l',\n",
       " 'brooks',\n",
       " 'brooks',\n",
       " 'gets',\n",
       " 'award',\n",
       " 'worthy',\n",
       " 'performances',\n",
       " 'entire',\n",
       " 'cast',\n",
       " 'winner',\n",
       " 'every',\n",
       " 'aspect',\n",
       " 'truly',\n",
       " 'delicious',\n",
       " 'slice',\n",
       " 'cyncial',\n",
       " 'life']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def normalise(wordlist):\n",
    "    lowered=[word.lower() for word in wordlist] #don't actually need this as already lowered\n",
    "    filtered=[word for word in lowered if word.isalpha() and word not in stop]\n",
    "    return filtered\n",
    "\n",
    "normalise(training[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'melvin': 6, 'simon': 4, 'dog': 4, 'playing': 3, 'one': 2, 'day': 2, 'serve': 2, 'carol': 2, 'threatens': 2, 'shut': 2, ...}),\n",
       " 'pos')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_norm = []\n",
    "for wordlist,label in training:\n",
    "    normalized_wl = normalise(wordlist)\n",
    "    fd_wl = FreqDist(normalized_wl)\n",
    "    training_norm.append((fd_wl,label))\n",
    "\n",
    "training_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5293,
     "status": "ok",
     "timestamp": 1633608866497,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "FLx5ja1aqe5U",
    "outputId": "578d2d71-4120-4185-9667-aee2af26127d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FreqDist({'melvin': 6, 'simon': 4, 'dog': 4, 'playing': 3, 'one': 2, 'day': 2, 'serve': 2, 'carol': 2, 'threatens': 2, 'shut': 2, ...}),\n",
       " 'pos')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in training]\n",
    "testing_norm=[(FreqDist(normalise(wordlist)),label) for (wordlist,label) in testing]\n",
    "\n",
    "training_norm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTGHJWSd7iQ5"
   },
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "572x5pEP7iQ6"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1633608876499,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "RPzluDd-7iQ6"
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\", \"excellent\"] # extend this one or put your own list here\n",
    "my_negative_word_list = [\"bad\", \"terrible\", \"awful\", \"dreadful\"] # extend this one or put your own list here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nQhC0Zaqe5W"
   },
   "source": [
    "Now lets see how often each of those words occurs in total in our positive and negative training data.  First, lets create a total of the FreqDists for positive data and for negative data.  As these are FreqDists (rather than simple dictionaries), we can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2058,
     "status": "ok",
     "timestamp": 1633608883704,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "7Tg_0bttqe5X",
    "outputId": "c8c6c20b-b2ba-443f-ab16-a7f13989412c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 3737, 'one': 2127, 'movie': 1721, 'like': 1285, 'story': 893, 'time': 882, 'good': 859, 'also': 848, 'even': 804, 'well': 762, ...})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq_dist=FreqDist()\n",
    "neg_freq_dist=FreqDist()\n",
    "\n",
    "for reviewDist,label in training_norm:\n",
    "    if label=='pos':\n",
    "        pos_freq_dist+=reviewDist\n",
    "    else:\n",
    "        neg_freq_dist+=reviewDist\n",
    "        \n",
    "pos_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "JNUtrGjVqe5Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq_dist['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_freq_dist['bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad -473\n"
     ]
    }
   ],
   "source": [
    "words=['bad']\n",
    "\n",
    "for word in words:\n",
    "    diff=pos_freq_dist[word]-neg_freq_dist[word]\n",
    "    print(word,diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW9BMqP37iRH"
   },
   "source": [
    "### Exercise 3.2\n",
    "In the blank code cell below write code that uses the total frequency distributions `pos_freq_dist` and `neg_freq_dist` and the word lists `my_positive_word_list` and `my_negative_word_list` created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n",
    "\n",
    "You could display your findings in a table using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1633608893391,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "JPRJHuET7iRH"
   },
   "outputs": [],
   "source": [
    "def check_expectations(a_word_list,expectation,pos=pos_freq_dist,neg=neg_freq_dist):\n",
    "#expectation is a positive number if words are expected to be positive\n",
    "#expectation is a negative number if words are expected to be negative\n",
    "    results=[]\n",
    "    for word in a_word_list:\n",
    "        #print( f'CHECKING {word}\\n----------------------------------------------\\n' )\n",
    "        pos_freq=pos.get(word,0)\n",
    "        neg_freq=neg.get(word,0)\n",
    "        diff=pos_freq-neg_freq\n",
    "        #print( f'Positive frequency is {pos_freq} and negative frequency is {neg_freq}, and the frequency difference is {diff}\\n----------------------------------------------\\n' )\n",
    "        if diff*expectation>0:\n",
    "            print(\"As expected: for {} difference is {}\\n----------------------------------------------\\n\".format(word,diff))\n",
    "            results.append((word,diff,'yes'))\n",
    "        else:\n",
    "            print(\"Contrary to expectations: for {} difference is {}\\n----------------------------------------------\\n\".format(word,diff))\n",
    "            results.append((word,diff,'no'))\n",
    "            \n",
    "    return results\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1633608896088,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "b4aR84LE7iRL",
    "outputId": "2f159b33-19c5-476b-fba1-726c36a64f59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected: for good difference is 52\n",
      "----------------------------------------------\n",
      "\n",
      "As expected: for great difference is 254\n",
      "----------------------------------------------\n",
      "\n",
      "Contrary to expectations: for lovely difference is 0\n",
      "----------------------------------------------\n",
      "\n",
      "As expected: for excellent difference is 73\n",
      "----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results=check_expectations(my_positive_word_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1633608897055,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "bkfwFU4v7iRO",
    "outputId": "8241f257-ddd8-40fb-fd3f-a0ba19d4f9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected: for bad difference is -473\n",
      "----------------------------------------------\n",
      "\n",
      "As expected: for terrible difference is -54\n",
      "----------------------------------------------\n",
      "\n",
      "As expected: for awful difference is -60\n",
      "----------------------------------------------\n",
      "\n",
      "As expected: for dreadful difference is -6\n",
      "----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results+=check_expectations(my_negative_word_list,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1633608898115,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "OefXeFaKqe5b",
    "outputId": "6a445df4-8e32-4a74-b614-691090ccca0b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>diff</th>\n",
       "      <th>conforms to expectation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>52</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great</td>\n",
       "      <td>254</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>excellent</td>\n",
       "      <td>73</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>-473</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>terrible</td>\n",
       "      <td>-54</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>awful</td>\n",
       "      <td>-60</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dreadful</td>\n",
       "      <td>-6</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word  diff conforms to expectation\n",
       "0       good    52                     yes\n",
       "1      great   254                     yes\n",
       "2     lovely     0                      no\n",
       "3  excellent    73                     yes\n",
       "4        bad  -473                     yes\n",
       "5   terrible   -54                     yes\n",
       "6      awful   -60                     yes\n",
       "7   dreadful    -6                     yes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(results,columns=['word','diff','conforms to expectation'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bXExCJp7iRS"
   },
   "source": [
    "### Exercise 3.3\n",
    "Now, you are going to create positive and negative word lists automatically from the training data. In order to do this:\n",
    "\n",
    "1. write two new functions to help with automating the process of generating wordlists.\n",
    "\n",
    "    - `most_frequent_words` - this function should take THREE arguments: 2 frequency distributions and a natural number, k. It should order words by how much more they occur in one frequency distribution than the other.   It should then return the top k highest scoring words. You might want to use the `most_common` method from the `FreqDist` class - this returns a list of word, frequency pairs ordered by frequency.  You might also or alternatively want to use pythons built-in `sorted` function\n",
    "    - `words_above_threshold` - this function also takes three arguments: 2 frequency distributions and a natural number, k. Again, it should order words by how much more they occur in one distribution than the other.  It should return all of the words that have a score greater than k.\n",
    "\n",
    "2. Using the training data, create two sets of positive and negative word lists using these functions (1 set with each function). \n",
    "3.  Display these 4 lists (possibly in a `Pandas` dataframe?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1633608914683,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "GUxe8Ol07iRS",
    "outputId": "7c2c1ade-0b3d-4433-9b26-819696551496"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'film': 756, 'life': 384, 'also': 300, 'great': 254, 'story': 220, 'world': 216, 'many': 213, 'films': 212, 'best': 211, 'one': 194, ...})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdiff=pos_freq_dist-neg_freq_dist\n",
    "posdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1633608914929,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "6KwdjbI-7iRW",
    "outputId": "ce6182a0-2278-439a-95fc-0a01713268b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdiff.get('excellent',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1633608916133,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "0boOFfX8_DkE",
    "outputId": "7ee26087-2074-43ba-fecf-30c3e1ac8d4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdiff.get('good',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1633608916322,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "2ma9fpwGSQyz",
    "outputId": "e860f0cb-ff04-4bfb-813c-914cf9a6d474"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 756),\n",
       " ('life', 384),\n",
       " ('also', 300),\n",
       " ('great', 254),\n",
       " ('story', 220),\n",
       " ('world', 216),\n",
       " ('many', 213),\n",
       " ('films', 212),\n",
       " ('best', 211),\n",
       " ('one', 194),\n",
       " ('well', 184),\n",
       " ('family', 157),\n",
       " ('american', 157),\n",
       " ('jackie', 137),\n",
       " ('first', 134),\n",
       " ('quite', 128),\n",
       " ('although', 128),\n",
       " ('performance', 125),\n",
       " ('war', 125),\n",
       " ('young', 112),\n",
       " ('way', 111),\n",
       " ('men', 111),\n",
       " ('however', 110),\n",
       " ('new', 109),\n",
       " ('see', 109),\n",
       " ('mother', 108),\n",
       " ('john', 104),\n",
       " ('seen', 104),\n",
       " ('job', 104),\n",
       " ('star', 103),\n",
       " ('people', 101),\n",
       " ('love', 101),\n",
       " ('perfect', 99),\n",
       " ('takes', 97),\n",
       " ('different', 97),\n",
       " ('day', 96),\n",
       " ('always', 92),\n",
       " ('may', 91),\n",
       " ('true', 91),\n",
       " ('disney', 91),\n",
       " ('yet', 90),\n",
       " ('often', 90),\n",
       " ('dark', 90),\n",
       " ('man', 89),\n",
       " ('years', 88),\n",
       " ('gives', 86),\n",
       " ('especially', 85),\n",
       " ('makes', 85),\n",
       " ('black', 85),\n",
       " ('time', 83),\n",
       " ('city', 83),\n",
       " ('cameron', 83),\n",
       " ('father', 82),\n",
       " ('fiction', 82),\n",
       " ('performances', 80),\n",
       " ('still', 79),\n",
       " ('without', 78),\n",
       " ('wars', 78),\n",
       " ('truman', 78),\n",
       " ('horror', 77),\n",
       " ('characters', 76),\n",
       " ('beautiful', 76),\n",
       " ('wonderful', 76),\n",
       " ('flynt', 76),\n",
       " ('us', 75),\n",
       " ('perfectly', 74),\n",
       " ('excellent', 73),\n",
       " ('music', 72),\n",
       " ('strong', 72),\n",
       " ('toy', 71),\n",
       " ('almost', 70),\n",
       " ('right', 70),\n",
       " ('effective', 69),\n",
       " ('animation', 68),\n",
       " ('sweet', 68),\n",
       " ('several', 67),\n",
       " ('picture', 67),\n",
       " ('sense', 67),\n",
       " ('role', 66),\n",
       " ('final', 66),\n",
       " ('much', 66),\n",
       " ('throughout', 66),\n",
       " ('hilarious', 66),\n",
       " ('begins', 66),\n",
       " ('simple', 65),\n",
       " ('human', 64),\n",
       " ('making', 63),\n",
       " ('town', 63),\n",
       " ('sometimes', 63),\n",
       " ('old', 62),\n",
       " ('child', 62),\n",
       " ('titanic', 62),\n",
       " ('including', 61),\n",
       " ('character', 61),\n",
       " ('master', 61),\n",
       " ('others', 60),\n",
       " ('science', 60),\n",
       " ('works', 59),\n",
       " ('change', 58),\n",
       " ('year', 58),\n",
       " ('chan', 58),\n",
       " ('memorable', 58),\n",
       " ('relationship', 57),\n",
       " ('solid', 56),\n",
       " ('small', 56),\n",
       " ('david', 56),\n",
       " ('high', 55),\n",
       " ('george', 55),\n",
       " ('view', 55),\n",
       " ('death', 55),\n",
       " ('fun', 54),\n",
       " ('america', 54),\n",
       " ('ending', 54),\n",
       " ('frank', 54),\n",
       " ('side', 54),\n",
       " ('together', 53),\n",
       " ('tarantino', 53),\n",
       " ('good', 52),\n",
       " ('friends', 52),\n",
       " ('political', 52),\n",
       " ('tale', 52),\n",
       " ('wonderfully', 52),\n",
       " ('intelligent', 52),\n",
       " ('aliens', 52),\n",
       " ('beauty', 52),\n",
       " ('children', 51),\n",
       " ('heart', 51),\n",
       " ('brilliant', 51),\n",
       " ('mike', 51),\n",
       " ('though', 50),\n",
       " ('behind', 50),\n",
       " ('shows', 50),\n",
       " ('history', 50),\n",
       " ('voice', 50),\n",
       " ('alien', 50),\n",
       " ('overall', 49),\n",
       " ('reality', 49),\n",
       " ('powerful', 49),\n",
       " ('terrific', 49),\n",
       " ('back', 48),\n",
       " ('hollywood', 48),\n",
       " ('become', 48),\n",
       " ('game', 48),\n",
       " ('rather', 47),\n",
       " ('done', 47),\n",
       " ('able', 47),\n",
       " ('find', 47),\n",
       " ('friend', 47),\n",
       " ('extremely', 47),\n",
       " ('lucas', 47),\n",
       " ('mind', 46),\n",
       " ('amazing', 46),\n",
       " ('complex', 45),\n",
       " ('movies', 45),\n",
       " ('animated', 45),\n",
       " ('stories', 45),\n",
       " ('light', 45),\n",
       " ('henry', 45),\n",
       " ('wife', 44),\n",
       " ('visual', 44),\n",
       " ('dream', 44),\n",
       " ('perhaps', 43),\n",
       " ('nature', 43),\n",
       " ('boy', 43),\n",
       " ('art', 43),\n",
       " ('subtle', 43),\n",
       " ('period', 43),\n",
       " ('normal', 42),\n",
       " ('son', 42),\n",
       " ('simon', 42),\n",
       " ('home', 42),\n",
       " ('whose', 42),\n",
       " ('ever', 42),\n",
       " ('bit', 42),\n",
       " ('entertaining', 42),\n",
       " ('entertainment', 42),\n",
       " ('private', 42),\n",
       " ('jedi', 42),\n",
       " ('fascinating', 42),\n",
       " ('scorsese', 42),\n",
       " ('similar', 41),\n",
       " ('everything', 41),\n",
       " ('realistic', 41),\n",
       " ('parents', 41),\n",
       " ('future', 41),\n",
       " ('form', 41),\n",
       " ('rocky', 41),\n",
       " ('greatest', 41),\n",
       " ('somewhat', 41),\n",
       " ('pulp', 41),\n",
       " ('masterpiece', 41),\n",
       " ('outstanding', 41),\n",
       " ('vincent', 41),\n",
       " ('brown', 40),\n",
       " ('rich', 40),\n",
       " ('leads', 40),\n",
       " ('robert', 40),\n",
       " ('definitely', 40),\n",
       " ('classic', 40),\n",
       " ('shot', 40),\n",
       " ('allows', 40),\n",
       " ('class', 40),\n",
       " ('enjoy', 40),\n",
       " ('emotional', 40),\n",
       " ('novel', 40),\n",
       " ('culture', 40),\n",
       " ('gattaca', 40),\n",
       " ('lebowski', 40),\n",
       " ('less', 39),\n",
       " ('among', 39),\n",
       " ('husband', 39),\n",
       " ('finds', 39),\n",
       " ('sets', 39),\n",
       " ('group', 39),\n",
       " ('genre', 39),\n",
       " ('attention', 39),\n",
       " ('surprisingly', 39),\n",
       " ('edge', 39),\n",
       " ('natural', 39),\n",
       " ('sees', 39),\n",
       " ('named', 39),\n",
       " ('flaws', 39),\n",
       " ('experience', 38),\n",
       " ('things', 38),\n",
       " ('strange', 38),\n",
       " ('unique', 38),\n",
       " ('moments', 38),\n",
       " ('adult', 38),\n",
       " ('upon', 38),\n",
       " ('spielberg', 38),\n",
       " ('journey', 38),\n",
       " ('self', 38),\n",
       " ('cinema', 38),\n",
       " ('cauldron', 38),\n",
       " ('brooks', 37),\n",
       " ('despite', 37),\n",
       " ('feeling', 37),\n",
       " ('era', 37),\n",
       " ('audiences', 37),\n",
       " ('follow', 37),\n",
       " ('particularly', 37),\n",
       " ('ben', 37),\n",
       " ('jar', 37),\n",
       " ('mulan', 37),\n",
       " ('artist', 36),\n",
       " ('oscar', 36),\n",
       " ('evil', 36),\n",
       " ('moving', 36),\n",
       " ('show', 36),\n",
       " ('scene', 36),\n",
       " ('deal', 36),\n",
       " ('superb', 36),\n",
       " ('easy', 36),\n",
       " ('slowly', 36),\n",
       " ('shakespeare', 36),\n",
       " ('today', 35),\n",
       " ('bill', 35),\n",
       " ('turn', 35),\n",
       " ('images', 35),\n",
       " ('whale', 35),\n",
       " ('told', 35),\n",
       " ('modern', 35),\n",
       " ('drunken', 35),\n",
       " ('emotions', 35),\n",
       " ('approach', 35),\n",
       " ('mood', 35),\n",
       " ('hanks', 35),\n",
       " ('trek', 35),\n",
       " ('order', 34),\n",
       " ('place', 34),\n",
       " ('enjoyable', 34),\n",
       " ('deserves', 34),\n",
       " ('eventually', 34),\n",
       " ('princess', 34),\n",
       " ('share', 34),\n",
       " ('towards', 34),\n",
       " ('white', 34),\n",
       " ('robocop', 34),\n",
       " ('issues', 34),\n",
       " ('austin', 34),\n",
       " ('touching', 34),\n",
       " ('italian', 34),\n",
       " ('woody', 34),\n",
       " ('adventure', 34),\n",
       " ('altman', 34),\n",
       " ('fantastic', 34),\n",
       " ('ordell', 34),\n",
       " ('british', 33),\n",
       " ('van', 33),\n",
       " ('supporting', 33),\n",
       " ('personal', 33),\n",
       " ('fine', 33),\n",
       " ('happy', 33),\n",
       " ('soldiers', 33),\n",
       " ('seeing', 33),\n",
       " ('previous', 33),\n",
       " ('feel', 33),\n",
       " ('freedom', 33),\n",
       " ('setting', 33),\n",
       " ('bond', 33),\n",
       " ('powers', 33),\n",
       " ('matrix', 33),\n",
       " ('marriage', 33),\n",
       " ('larry', 33),\n",
       " ('menace', 33),\n",
       " ('songs', 32),\n",
       " ('use', 32),\n",
       " ('finest', 32),\n",
       " ('finally', 32),\n",
       " ('release', 32),\n",
       " ('image', 32),\n",
       " ('using', 32),\n",
       " ('dramatic', 32),\n",
       " ('social', 32),\n",
       " ('became', 32),\n",
       " ('direction', 32),\n",
       " ('details', 32),\n",
       " ('ghost', 32),\n",
       " ('boogie', 32),\n",
       " ('anderson', 32),\n",
       " ('nicholson', 31),\n",
       " ('certainly', 31),\n",
       " ('pace', 31),\n",
       " ('screen', 31),\n",
       " ('law', 31),\n",
       " ('portrayal', 31),\n",
       " ('martin', 31),\n",
       " ('unlike', 31),\n",
       " ('follows', 31),\n",
       " ('impressive', 31),\n",
       " ('united', 31),\n",
       " ('german', 31),\n",
       " ('whether', 31),\n",
       " ('elements', 31),\n",
       " ('army', 31),\n",
       " ('keep', 31),\n",
       " ('aspects', 31),\n",
       " ('bowfinger', 31),\n",
       " ('leila', 31),\n",
       " ('apartment', 30),\n",
       " ('court', 30),\n",
       " ('help', 30),\n",
       " ('highly', 30),\n",
       " ('cold', 30),\n",
       " ('version', 30),\n",
       " ('key', 30),\n",
       " ('lives', 30),\n",
       " ('viewer', 30),\n",
       " ('hong', 30),\n",
       " ('epic', 30),\n",
       " ('person', 30),\n",
       " ('rush', 30),\n",
       " ('washington', 30),\n",
       " ('queen', 30),\n",
       " ('event', 30),\n",
       " ('japanese', 30),\n",
       " ('caught', 30),\n",
       " ('sweetback', 30),\n",
       " ('damon', 30),\n",
       " ('force', 29),\n",
       " ('along', 29),\n",
       " ('dance', 29),\n",
       " ('turns', 29),\n",
       " ('due', 29),\n",
       " ('present', 29),\n",
       " ('colors', 29),\n",
       " ('frightening', 29),\n",
       " ('destination', 29),\n",
       " ('la', 29),\n",
       " ('opens', 29),\n",
       " ('remains', 29),\n",
       " ('fu', 29),\n",
       " ('surprising', 29),\n",
       " ('provides', 29),\n",
       " ('created', 29),\n",
       " ('poker', 29),\n",
       " ('guido', 29),\n",
       " ('niro', 29),\n",
       " ('toys', 29),\n",
       " ('carlito', 29),\n",
       " ('little', 28),\n",
       " ('themes', 28),\n",
       " ('dillon', 28),\n",
       " ('knows', 28),\n",
       " ('believe', 28),\n",
       " ('tells', 28),\n",
       " ('usually', 28),\n",
       " ('seem', 28),\n",
       " ('carter', 28),\n",
       " ('william', 28),\n",
       " ('teacher', 28),\n",
       " ('left', 28),\n",
       " ('former', 28),\n",
       " ('states', 28),\n",
       " ('state', 28),\n",
       " ('process', 28),\n",
       " ('computer', 28),\n",
       " ('musical', 28),\n",
       " ('battle', 28),\n",
       " ('king', 28),\n",
       " ('cusack', 28),\n",
       " ('pain', 28),\n",
       " ('justice', 28),\n",
       " ('phantom', 28),\n",
       " ('anna', 28),\n",
       " ('forces', 28),\n",
       " ('malkovich', 28),\n",
       " ('crowe', 28),\n",
       " ('stunning', 27),\n",
       " ('martial', 27),\n",
       " ('rob', 27),\n",
       " ('remarkable', 27),\n",
       " ('cinematography', 27),\n",
       " ('amistad', 27),\n",
       " ('lies', 27),\n",
       " ('compelling', 27),\n",
       " ('chris', 27),\n",
       " ('smart', 27),\n",
       " ('kung', 27),\n",
       " ('fox', 27),\n",
       " ('uses', 27),\n",
       " ('antz', 27),\n",
       " ('mature', 27),\n",
       " ('truth', 27),\n",
       " ('hamlet', 27),\n",
       " ('derek', 27),\n",
       " ('harris', 27),\n",
       " ('never', 26),\n",
       " ('learns', 26),\n",
       " ('cast', 26),\n",
       " ('ride', 26),\n",
       " ('certain', 26),\n",
       " ('four', 26),\n",
       " ('religion', 26),\n",
       " ('released', 26),\n",
       " ('brings', 26),\n",
       " ('scream', 26),\n",
       " ('score', 26),\n",
       " ('friendship', 26),\n",
       " ('satisfying', 26),\n",
       " ('incredible', 26),\n",
       " ('believes', 26),\n",
       " ('touch', 26),\n",
       " ('attacks', 26),\n",
       " ('tony', 26),\n",
       " ('driver', 26),\n",
       " ('taran', 26),\n",
       " ('seems', 25),\n",
       " ('president', 25),\n",
       " ('comic', 25),\n",
       " ('entire', 25),\n",
       " ('truly', 25),\n",
       " ('singer', 25),\n",
       " ('must', 25),\n",
       " ('returns', 25),\n",
       " ('song', 25),\n",
       " ('alex', 25),\n",
       " ('dude', 25),\n",
       " ('country', 25),\n",
       " ('clearly', 25),\n",
       " ('portrayed', 25),\n",
       " ('success', 25),\n",
       " ('nevertheless', 25),\n",
       " ('tragedy', 25),\n",
       " ('fei', 25),\n",
       " ('nicely', 25),\n",
       " ('famous', 25),\n",
       " ('perspective', 25),\n",
       " ('choice', 25),\n",
       " ('safe', 25),\n",
       " ('mysterious', 25),\n",
       " ('asks', 25),\n",
       " ('technology', 25),\n",
       " ('foster', 25),\n",
       " ('nomination', 25),\n",
       " ('nights', 25),\n",
       " ('burton', 25),\n",
       " ('historical', 25),\n",
       " ('graham', 25),\n",
       " ('fargo', 25),\n",
       " ('egoyan', 25),\n",
       " ('meets', 24),\n",
       " ('soon', 24),\n",
       " ('agent', 24),\n",
       " ('design', 24),\n",
       " ('away', 24),\n",
       " ('stage', 24),\n",
       " ('presented', 24),\n",
       " ('red', 24),\n",
       " ('eyes', 24),\n",
       " ('create', 24),\n",
       " ('sure', 24),\n",
       " ('rare', 24),\n",
       " ('chad', 24),\n",
       " ('tom', 24),\n",
       " ('bringing', 24),\n",
       " ('english', 24),\n",
       " ('chemistry', 24),\n",
       " ('refreshing', 24),\n",
       " ('experiences', 24),\n",
       " ('theme', 24),\n",
       " ('search', 24),\n",
       " ('equally', 24),\n",
       " ('felt', 24),\n",
       " ('de', 24),\n",
       " ('diaz', 24),\n",
       " ('buzz', 24),\n",
       " ('frankenstein', 24),\n",
       " ('lets', 24),\n",
       " ('important', 24),\n",
       " ('holds', 24),\n",
       " ('alice', 24),\n",
       " ('filmmaking', 24),\n",
       " ('fully', 24),\n",
       " ('spacey', 24),\n",
       " ('captures', 24),\n",
       " ('obi', 24),\n",
       " ('quiet', 24),\n",
       " ('rated', 23),\n",
       " ('blue', 23),\n",
       " ('leave', 23),\n",
       " ('local', 23),\n",
       " ('segment', 23),\n",
       " ('visually', 23),\n",
       " ('atmosphere', 23),\n",
       " ('constantly', 23),\n",
       " ('movement', 23),\n",
       " ('put', 23),\n",
       " ('co', 23),\n",
       " ('enjoyed', 23),\n",
       " ('moral', 23),\n",
       " ('joy', 23),\n",
       " ('coen', 23),\n",
       " ('ordinary', 23),\n",
       " ('endearing', 23),\n",
       " ('easily', 23),\n",
       " ('iron', 23),\n",
       " ('delightful', 23),\n",
       " ('fate', 23),\n",
       " ('wedding', 23),\n",
       " ('likeable', 23),\n",
       " ('slightly', 23),\n",
       " ('shall', 23),\n",
       " ('rose', 23),\n",
       " ('proves', 23),\n",
       " ('streets', 23),\n",
       " ('benigni', 23),\n",
       " ('jude', 23),\n",
       " ('magnificent', 23),\n",
       " ('argento', 23),\n",
       " ('bulworth', 23),\n",
       " ('nello', 23),\n",
       " ('worthy', 22),\n",
       " ('dog', 22),\n",
       " ('honest', 22),\n",
       " ('school', 22),\n",
       " ('intense', 22),\n",
       " ('murder', 22),\n",
       " ('terms', 22),\n",
       " ('production', 22),\n",
       " ('brought', 22),\n",
       " ('wong', 22),\n",
       " ('x', 22),\n",
       " ('age', 22),\n",
       " ('surprised', 22),\n",
       " ('loose', 22),\n",
       " ('past', 22),\n",
       " ('return', 22),\n",
       " ('views', 22),\n",
       " ('color', 22),\n",
       " ('drama', 22),\n",
       " ('based', 22),\n",
       " ('second', 22),\n",
       " ('non', 22),\n",
       " ('sister', 22),\n",
       " ('anger', 22),\n",
       " ('creates', 22),\n",
       " ('anti', 22),\n",
       " ('forced', 22),\n",
       " ('younger', 22),\n",
       " ('breathtaking', 22),\n",
       " ('contact', 22),\n",
       " ('near', 22),\n",
       " ('trilogy', 22),\n",
       " ('ted', 22),\n",
       " ('stiller', 22),\n",
       " ('extraordinary', 22),\n",
       " ('fits', 22),\n",
       " ('finn', 22),\n",
       " ('danny', 22),\n",
       " ('qui', 22),\n",
       " ('stolen', 22),\n",
       " ('ethan', 22),\n",
       " ('steals', 22),\n",
       " ('mitchell', 22),\n",
       " ('angels', 22),\n",
       " ('fable', 22),\n",
       " ('traditional', 22),\n",
       " ('schindler', 22),\n",
       " ('empire', 22),\n",
       " ('pam', 22),\n",
       " ('matilda', 22),\n",
       " ('carol', 21),\n",
       " ('gay', 21),\n",
       " ('james', 21),\n",
       " ('keeps', 21),\n",
       " ('working', 21),\n",
       " ('stuart', 21),\n",
       " ('risk', 21),\n",
       " ('ali', 21),\n",
       " ('violence', 21),\n",
       " ('beautifully', 21),\n",
       " ('judge', 21),\n",
       " ('succeeds', 21),\n",
       " ('genuine', 21),\n",
       " ('peter', 21),\n",
       " ('accident', 21),\n",
       " ('controversial', 21),\n",
       " ('hidden', 21),\n",
       " ('led', 21),\n",
       " ('subject', 21),\n",
       " ('nuclear', 21),\n",
       " ('october', 21),\n",
       " ('unusual', 21),\n",
       " ('designer', 21),\n",
       " ('flawless', 21),\n",
       " ('wan', 21),\n",
       " ('gon', 21),\n",
       " ('anakin', 21),\n",
       " ('mouse', 21),\n",
       " ('jesus', 21),\n",
       " ('homer', 21),\n",
       " ('coens', 21),\n",
       " ('thirteenth', 21),\n",
       " ('rounders', 21),\n",
       " ('kidman', 21),\n",
       " ('lumumba', 21),\n",
       " ('power', 20),\n",
       " ('hall', 20),\n",
       " ('various', 20),\n",
       " ('matt', 20),\n",
       " ('take', 20),\n",
       " ('far', 20),\n",
       " ('feels', 20),\n",
       " ('comes', 20),\n",
       " ('difficult', 20),\n",
       " ('system', 20),\n",
       " ('real', 20),\n",
       " ('strength', 20),\n",
       " ('initially', 20),\n",
       " ('blood', 20),\n",
       " ('camille', 20),\n",
       " ('events', 20),\n",
       " ('remembered', 20),\n",
       " ('legal', 20),\n",
       " ('chinese', 20),\n",
       " ('government', 20),\n",
       " ('howard', 20),\n",
       " ('society', 20),\n",
       " ('emotionally', 20),\n",
       " ('contrast', 20),\n",
       " ('soul', 20),\n",
       " ('hereafter', 20),\n",
       " ('delivers', 20),\n",
       " ('raised', 20),\n",
       " ('typical', 20),\n",
       " ('deeply', 20),\n",
       " ('expectations', 20),\n",
       " ('sequence', 20),\n",
       " ('directing', 20),\n",
       " ('norton', 20),\n",
       " ('kubrick', 20),\n",
       " ('seat', 20),\n",
       " ('minor', 20),\n",
       " ('childhood', 20),\n",
       " ('joe', 20),\n",
       " ('winslet', 20),\n",
       " ('paced', 20),\n",
       " ('primary', 20),\n",
       " ('ford', 20),\n",
       " ('mermaid', 20),\n",
       " ('tucker', 20),\n",
       " ('lambeau', 20),\n",
       " ('mamet', 20),\n",
       " ('melvin', 19),\n",
       " ('inside', 19),\n",
       " ('u', 19),\n",
       " ('known', 19),\n",
       " ('bob', 19),\n",
       " ('scenes', 19),\n",
       " ('ryan', 19),\n",
       " ('twists', 19),\n",
       " ('visuals', 19),\n",
       " ('al', 19),\n",
       " ('imaginative', 19),\n",
       " ('times', 19),\n",
       " ('intensity', 19),\n",
       " ('exciting', 19),\n",
       " ('loved', 19),\n",
       " ('telling', 19),\n",
       " ('charming', 19),\n",
       " ('means', 19),\n",
       " ('struggle', 19),\n",
       " ('humanity', 19),\n",
       " ('ways', 19),\n",
       " ('faced', 19),\n",
       " ('everyday', 19),\n",
       " ('born', 19),\n",
       " ('clever', 19),\n",
       " ('choices', 19),\n",
       " ('disturbing', 19),\n",
       " ('notch', 19),\n",
       " ('bobby', 19),\n",
       " ('castle', 19),\n",
       " ('race', 19),\n",
       " ('footage', 19),\n",
       " ('expect', 19),\n",
       " ('study', 19),\n",
       " ('daniel', 19),\n",
       " ('delight', 19),\n",
       " ('soundtrack', 19),\n",
       " ('politics', 19),\n",
       " ('paxton', 19),\n",
       " ('darth', 19),\n",
       " ('jerome', 19),\n",
       " ('revolution', 19),\n",
       " ('andy', 19),\n",
       " ('julianne', 19),\n",
       " ('ed', 19),\n",
       " ('clooney', 19),\n",
       " ('christ', 19),\n",
       " ('boiler', 19),\n",
       " ('mulder', 19),\n",
       " ('marty', 19),\n",
       " ('l', 18),\n",
       " ('pop', 18),\n",
       " ('fairly', 18),\n",
       " ('russell', 18),\n",
       " ('winning', 18),\n",
       " ('indeed', 18),\n",
       " ('radio', 18),\n",
       " ('complicated', 18),\n",
       " ('bring', 18),\n",
       " ('eight', 18),\n",
       " ('tradition', 18),\n",
       " ('feature', 18),\n",
       " ('therefore', 18),\n",
       " ('enter', 18),\n",
       " ('clear', 18),\n",
       " ('kind', 18),\n",
       " ('hands', 18),\n",
       " ('leaving', 18),\n",
       " ('africans', 18),\n",
       " ('grand', 18),\n",
       " ('nearly', 18),\n",
       " ('seemingly', 18),\n",
       " ('watch', 18),\n",
       " ('stark', 18),\n",
       " ('troubled', 18),\n",
       " ('beginning', 18),\n",
       " ('loves', 18),\n",
       " ('building', 18),\n",
       " ('bitter', 18),\n",
       " ('reading', 18),\n",
       " ('spirit', 18),\n",
       " ('realizes', 18),\n",
       " ('ian', 18),\n",
       " ('remain', 18),\n",
       " ('detail', 18),\n",
       " ('element', 18),\n",
       " ('emperor', 18),\n",
       " ('thrilling', 18),\n",
       " ('floor', 18),\n",
       " ('moves', 18),\n",
       " ('lord', 18),\n",
       " ('hits', 18),\n",
       " ('shop', 18),\n",
       " ('faith', 18),\n",
       " ('dreamworks', 18),\n",
       " ('fashioned', 18),\n",
       " ('niccol', 18),\n",
       " ('dolores', 18),\n",
       " ('invasion', 18),\n",
       " ('bubby', 18),\n",
       " ('witty', 17),\n",
       " ('stephen', 17),\n",
       " ('added', 17),\n",
       " ('segments', 17),\n",
       " ('plenty', 17),\n",
       " ('ensemble', 17),\n",
       " ('innocence', 17),\n",
       " ('thanks', 17),\n",
       " ('starship', 17),\n",
       " ('critics', 17),\n",
       " ('taking', 17),\n",
       " ('mary', 17),\n",
       " ('crime', 17),\n",
       " ('passion', 17),\n",
       " ('everyone', 17),\n",
       " ('fans', 17),\n",
       " ('worker', 17),\n",
       " ('continues', 17),\n",
       " ('captured', 17),\n",
       " ('edward', 17),\n",
       " ('food', 17),\n",
       " ('files', 17),\n",
       " ('plane', 17),\n",
       " ('shortly', 17),\n",
       " ('brother', 17),\n",
       " ('strike', 17),\n",
       " ('taken', 17),\n",
       " ('appreciate', 17),\n",
       " ('result', 17),\n",
       " ('river', 17),\n",
       " ('chance', 17),\n",
       " ('finding', 17),\n",
       " ('motion', 17),\n",
       " ('arts', 17),\n",
       " ('victor', 17),\n",
       " ('shine', 17),\n",
       " ('memory', 17),\n",
       " ('shocking', 17),\n",
       " ('provide', 17),\n",
       " ('stone', 17),\n",
       " ('colorful', 17),\n",
       " ('realize', 17),\n",
       " ('col', 17),\n",
       " ('discovers', 17),\n",
       " ('horizon', 17),\n",
       " ('doubt', 17),\n",
       " ('matter', 17),\n",
       " ('boys', 17),\n",
       " ('average', 17),\n",
       " ('heroine', 17),\n",
       " ('respectively', 17),\n",
       " ('shark', 17),\n",
       " ('develops', 17),\n",
       " ('miller', 17),\n",
       " ('arthur', 17),\n",
       " ('helps', 17),\n",
       " ('thankfully', 17),\n",
       " ('refuses', 17),\n",
       " ('knights', 17),\n",
       " ('decades', 17),\n",
       " ('wings', 17),\n",
       " ('loyal', 17),\n",
       " ('duke', 17),\n",
       " ('trade', 17),\n",
       " ('nicole', 17),\n",
       " ('zero', 17),\n",
       " ('motta', 17),\n",
       " ('fellini', 17),\n",
       " ('stewart', 17),\n",
       " ('vampire', 17),\n",
       " ('kiki', 17),\n",
       " ('destiny', 17),\n",
       " ('seth', 17),\n",
       " ('exotica', 17),\n",
       " ('frequency', 17),\n",
       " ('pollock', 17),\n",
       " ('bianca', 17),\n",
       " ('aspect', 16),\n",
       " ('fashion', 16),\n",
       " ('part', 16),\n",
       " ('available', 16),\n",
       " ('break', 16),\n",
       " ('million', 16),\n",
       " ('pure', 16),\n",
       " ('monty', 16),\n",
       " ('troopers', 16),\n",
       " ('voiced', 16),\n",
       " ('inspired', 16),\n",
       " ('ends', 16),\n",
       " ('dreams', 16),\n",
       " ('face', 16),\n",
       " ('married', 16),\n",
       " ('uncle', 16),\n",
       " ('ones', 16),\n",
       " ('academy', 16),\n",
       " ('wide', 16),\n",
       " ('innovative', 16),\n",
       " ('date', 16),\n",
       " ('parker', 16),\n",
       " ('finale', 16),\n",
       " ('criminal', 16),\n",
       " ('flawed', 16),\n",
       " ('lloyd', 16),\n",
       " ('individual', 16),\n",
       " ('storytelling', 16),\n",
       " ('traveling', 16),\n",
       " ('pie', 16),\n",
       " ('warm', 16),\n",
       " ('fantasy', 16),\n",
       " ('century', 16),\n",
       " ('concerned', 16),\n",
       " ('adults', 16),\n",
       " ('bug', 16),\n",
       " ('ripley', 16),\n",
       " ('recognize', 16),\n",
       " ('von', 16),\n",
       " ('vader', 16),\n",
       " ('jake', 16),\n",
       " ('stephane', 16),\n",
       " ('horner', 16),\n",
       " ('spencer', 16),\n",
       " ('kings', 16),\n",
       " ('marie', 16),\n",
       " ('shrek', 16),\n",
       " ('slade', 16),\n",
       " ('chocolat', 16),\n",
       " ('capone', 16),\n",
       " ('humbert', 16),\n",
       " ('reza', 16),\n",
       " ('romance', 15),\n",
       " ('favorite', 15),\n",
       " ('americans', 15),\n",
       " ('hours', 15),\n",
       " ('maintains', 15),\n",
       " ('following', 15),\n",
       " ('angry', 15),\n",
       " ('pulls', 15),\n",
       " ('changes', 15),\n",
       " ('portrait', 15),\n",
       " ('concept', 15),\n",
       " ('vision', 15),\n",
       " ('poignant', 15),\n",
       " ('extreme', 15),\n",
       " ('wang', 15),\n",
       " ('considered', 15),\n",
       " ('ultimately', 15),\n",
       " ('industry', 15),\n",
       " ('ambitious', 15),\n",
       " ('learn', 15),\n",
       " ('probably', 15),\n",
       " ('magic', 15),\n",
       " ('structure', 15),\n",
       " ('actor', 15),\n",
       " ('satire', 15),\n",
       " ('spectacular', 15),\n",
       " ('debate', 15),\n",
       " ('prisoners', 15),\n",
       " ('dragon', 15),\n",
       " ('successful', 15),\n",
       " ('pleasantville', 15),\n",
       " ('identify', 15),\n",
       " ('hurt', 15),\n",
       " ('afraid', 15),\n",
       " ('creation', 15),\n",
       " ('desire', 15),\n",
       " ('grown', 15),\n",
       " ('nice', 15),\n",
       " ('conversation', 15),\n",
       " ('spoken', 15),\n",
       " ('branagh', 15),\n",
       " ('maggie', 15),\n",
       " ('larger', 15),\n",
       " ('eccentric', 15),\n",
       " ('edition', 15),\n",
       " ('eat', 15),\n",
       " ('agrees', 15),\n",
       " ('sidney', 15),\n",
       " ('toward', 15),\n",
       " ('dread', 15),\n",
       " ('fonda', 15),\n",
       " ('win', 15),\n",
       " ('importance', 15),\n",
       " ('conventional', 15),\n",
       " ('consequences', 15),\n",
       " ('searching', 15),\n",
       " ('hop', 15),\n",
       " ('guilt', 15),\n",
       " ('artistic', 15),\n",
       " ('progresses', 15),\n",
       " ('pixar', 15),\n",
       " ('attorney', 15),\n",
       " ('influence', 15),\n",
       " ('wahlberg', 15),\n",
       " ('amidala', 15),\n",
       " ('skywalker', 15),\n",
       " ('terrorist', 15),\n",
       " ('denzel', 15),\n",
       " ('raging', 15),\n",
       " ('roman', 15),\n",
       " ('carver', 15),\n",
       " ('ideal', 15),\n",
       " ('lester', 15),\n",
       " ('cole', 15),\n",
       " ('fairy', 15),\n",
       " ('wallace', 15),\n",
       " ('egypt', 15),\n",
       " ('stephens', 15),\n",
       " ('rico', 15),\n",
       " ('treasure', 15),\n",
       " ('valjean', 15),\n",
       " ('kermit', 15),\n",
       " ('boone', 15),\n",
       " ('hedwig', 15),\n",
       " ('mummy', 15),\n",
       " ('opera', 14),\n",
       " ('albert', 14),\n",
       " ...]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posdiff.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1633608918288,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "o6hw9SFM7iRY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def most_frequent_words(posfreq,negfreq,topk):\n",
    "    difference=posfreq-negfreq\n",
    "    sorteddiff=difference.most_common()\n",
    "    justwords=[word for (word,freq) in sorteddiff[:topk]]\n",
    "    return justwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 471,
     "status": "ok",
     "timestamp": 1633608920761,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "jtwxYJmQ7iRb",
    "outputId": "fee9f014-069a-4c14-a2e0-3dce05421a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'life', 'also', 'great', 'story', 'world', 'many', 'films', 'best', 'one', 'well', 'family', 'american', 'jackie', 'first', 'quite', 'although', 'performance', 'war', 'young', 'way', 'men', 'however', 'new', 'see', 'mother', 'john', 'seen', 'job', 'star', 'people', 'love', 'perfect', 'takes', 'different', 'day', 'always', 'may', 'true', 'disney', 'yet', 'often', 'dark', 'man', 'years', 'gives', 'especially', 'makes', 'black', 'time']\n"
     ]
    }
   ],
   "source": [
    "top_pos=most_frequent_words(pos_freq_dist,neg_freq_dist,50)\n",
    "print(top_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1633608922493,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "kP84olqo7iRf",
    "outputId": "ddd49315-35a0-40f1-cd1f-424b1acca7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'bad', 'plot', 'worst', 'even', 'script', 'could', 'nothing', 'supposed', 'reason', 'get', 'boring', 'stupid', 'least', 'unfortunately', 'better', 'godzilla', 'harry', 'tv', 'know', 'big', 'minutes', 'maybe', 'got', 'looks', 'dull', 'tries', 'guy', 'batman', 'robin', 'thing', 'think', 'dialogue', 'west', 'waste', 'trying', 'wasted', 'mess', 'lame', 'seagal', 'minute', 'awful', 'action', 'half', 'give', 'made', 'worse', 'terrible', 'problem', 'oh']\n"
     ]
    }
   ],
   "source": [
    "top_neg=most_frequent_words(neg_freq_dist,pos_freq_dist,50)\n",
    "print(top_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1633608928311,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "ABq5Sb0j_j2p"
   },
   "outputs": [],
   "source": [
    "def above_threshold(posfreq,negfreq,threshold):\n",
    "  difference=posfreq-negfreq\n",
    "  sorteddiff=difference.most_common()\n",
    "  filtered=[w for (w,f) in sorteddiff if f>threshold]\n",
    "  return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187,
     "status": "ok",
     "timestamp": 1633608935597,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "idr8XYkWAmfl",
    "outputId": "a4c8db2b-9e3a-4ec6-8d57-45447f33a8b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'life', 'also', 'great', 'story', 'world', 'many', 'films', 'best', 'one', 'well', 'family', 'american', 'jackie', 'first', 'quite', 'although', 'performance', 'war', 'young', 'way', 'men', 'however', 'new', 'see', 'mother', 'john', 'seen', 'job', 'star', 'people', 'love']\n"
     ]
    }
   ],
   "source": [
    "above100pos = above_threshold(pos_freq_dist,neg_freq_dist,100)\n",
    "print(above100pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1633608941475,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "FaqsrbFTAmwI",
    "outputId": "5ae0a2a3-2e68-44e6-d8a1-614cdb754096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'bad', 'plot', 'worst', 'even', 'script', 'could', 'nothing', 'supposed', 'reason', 'get', 'boring', 'stupid']\n"
     ]
    }
   ],
   "source": [
    "above100neg = above_threshold(neg_freq_dist,pos_freq_dist,100)\n",
    "print(above100neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzVP-hJH7iRi"
   },
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier. \n",
    "> Make sure you understand the following code, which will be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 183,
     "status": "ok",
     "timestamp": 1633608949777,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "CSaLWU_A7iRi",
    "outputId": "4f5438d0-1bba-4177-cc7e-502cdacd1201"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "\n",
    "        # add code here that assigns an appropriate value to score\n",
    "        return \"neg\" if score < 0 else \"pos\"\n",
    "\n",
    "    ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
    "    #def classify_many(self, docs): \n",
    "    #    return [self.classify(doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"pos\", \"neg\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
    "classifier.classify(\"This movie was great\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irq5PVOc7iRl"
   },
   "source": [
    "### Exercise 3.1\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`. \n",
    "- For `score` less than 0, an \"`N`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`P`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1633608955437,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "0UXUyFHM7iRm",
    "outputId": "3626bc89-636f-4950-a93e-d6d96253963b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'great', 'lovely', 'excellent']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "        print(self._pos)\n",
    "\n",
    "    def classify(self, doc): \n",
    "        #doc is a FreqDist\n",
    "        score = 0\n",
    "        \n",
    "        # add code here that assigns an appropriate value to score\n",
    "        for word,value in doc.items():\n",
    "            if word in self._pos:\n",
    "                score+=value\n",
    "            if word in self._neg:\n",
    "                score-=value\n",
    "        \n",
    "        return \"neg\" if score < 0 else \"pos\" \n",
    "\n",
    "     ##we don't actually need to define the classify_many method as it is provided in ClassifierI\n",
    "    #def classify_many(self, docs): \n",
    "    #    return [self.classify(doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"pos\", \"neg\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(my_positive_word_list, my_negative_word_list)\n",
    "classifier.classify(FreqDist(\"This movie was dreadful\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "GRJWPhUF7iRo"
   },
   "source": [
    "### Exercise 3.2\n",
    "* Extend your SimpleClassifier class so that it has a `train` function which will derive the wordlists from training data.  You could build a separate class for each way of automatically deriving wordlists (which both inherit from SimpleClassifier) OR a single class which takes an extra parameter at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1633608963180,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "KT1PbIao7iRp"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier_mf(SimpleClassifier):\n",
    "    \n",
    "    def __init__(self,k):\n",
    "        self._k=k\n",
    "    \n",
    "    def train(self,training_data):\n",
    "        \n",
    "        pos_freq_dist=FreqDist()\n",
    "        neg_freq_dist=FreqDist()\n",
    "\n",
    "        for reviewDist,label in training_data:\n",
    "            if label=='pos':\n",
    "                pos_freq_dist+=reviewDist\n",
    "            else:\n",
    "                neg_freq_dist+=reviewDist\n",
    "                \n",
    "        self._pos=most_frequent_words(pos_freq_dist,neg_freq_dist,self._k)\n",
    "        self._neg=most_frequent_words(neg_freq_dist,pos_freq_dist,self._k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1633608964420,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "h85qqrOV7iRr"
   },
   "outputs": [],
   "source": [
    "movieclassifier=SimpleClassifier_mf(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "executionInfo": {
     "elapsed": 2081,
     "status": "ok",
     "timestamp": 1633608967598,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "3uM8En0_7iRu"
   },
   "outputs": [],
   "source": [
    "movieclassifier.train(training_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fWoIb8dDYzR"
   },
   "source": [
    "Try out your classifier on the test data.  We will look at how to evaluate classifiers in the next part, but in an ideal world, most of the positive test items will have been classified as 'P' and most of the negative test items will have been classified as 'N'.  Note that the batch_classify method takes a list of unlabelled documents so you can't give it a list of pairs (where each pair is doc and a label).  You can either use a list comprehension or the <code>zip(*list_of_pairs)</code> function to split a list of pairs into a pair of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1633608970973,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "s6fQs92SVu39",
    "outputId": "798a7b95-6886-43f1-c8ce-d2824798520c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieclassifier.classify(FreqDist(\"I hated this movie\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1633608972906,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "vFCEF0rx7iRx",
    "outputId": "1c69544a-c8c3-4b72-91aa-0ae944d445a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing,labels=zip(*testing_norm)\n",
    "movieclassifier.classify_many(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1633608972907,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "Us0zxFpsqe5g"
   },
   "outputs": [],
   "source": [
    "class SimpleClassifier_ot(SimpleClassifier):\n",
    "    \n",
    "    def __init__(self,k):\n",
    "        self._k=k\n",
    "    \n",
    "    def train(self,training_data):\n",
    "        \n",
    "        pos_freq_dist=FreqDist()\n",
    "        neg_freq_dist=FreqDist()\n",
    "\n",
    "        for reviewDist,label in training_data:\n",
    "            if label=='pos':\n",
    "                pos_freq_dist+=reviewDist\n",
    "            else:\n",
    "                neg_freq_dist+=reviewDist\n",
    "                \n",
    "        self._pos=above_threshold(pos_freq_dist,neg_freq_dist,self._k)\n",
    "        self._neg=above_threshold(neg_freq_dist,pos_freq_dist,self._k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1633608975930,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "i9MTBKOuDPU9"
   },
   "outputs": [],
   "source": [
    "movieclassifier2=SimpleClassifier_ot(50)\n",
    "movieclassifier2.train(training_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1633608976248,
     "user": {
      "displayName": "Julie Weeds",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13844540934373660130"
     },
     "user_tz": -60
    },
    "id": "yuS6Urkaqe5g",
    "outputId": "6dab290c-d039-4c14-80bc-f39173b3057e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos',\n",
       " 'pos']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieclassifier2.classify_many(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nphj7NkSqe5g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_3_1_SOLUTIONS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
